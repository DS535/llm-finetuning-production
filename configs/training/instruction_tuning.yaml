# Instruction Tuning Training Configuration
# Optimized for 8GB GPU

# Dataset
dataset_name: "databricks/databricks-dolly-15k"
dataset_type: "dolly"
prompt_template: "alpaca"

# Training Hyperparameters
num_epochs: 3
batch_size: 4
gradient_accumulation_steps: 4    # Effective batch size = 16
learning_rate: 2.0e-4             # Higher LR for LoRA
warmup_steps: 100
max_length: 512

# Optimization
fp16: true                         # Use fp16 for T4 GPU
bf16: false                        # Use bf16 for A100
gradient_checkpointing: true
optim: "paged_adamw_8bit"         # 8-bit optimizer

# Logging & Checkpointing
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 2

# Scheduler
lr_scheduler_type: "cosine"
weight_decay: 0.01

# Evaluation
evaluation_strategy: "steps"
load_best_model_at_end: true
metric_for_best_model: "loss"
