{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Complete LLM Finetuning Tutorial - Master Notebook\n",
    "\n",
    "**Everything you need to learn LLM finetuning in ONE notebook!**\n",
    "\n",
    "This comprehensive notebook covers:\n",
    "1. ‚úÖ Environment Setup\n",
    "2. ‚úÖ Data Exploration (Multiple Datasets)\n",
    "3. ‚úÖ Baseline Evaluation\n",
    "4. ‚úÖ Full Finetuning (GPT-2)\n",
    "5. ‚úÖ LoRA Finetuning (Parameter-Efficient)\n",
    "6. ‚úÖ QLoRA (7B Model on 8GB GPU!)\n",
    "7. ‚úÖ Instruction Tuning (ChatGPT-style)\n",
    "8. ‚úÖ Text Classification (BERT)\n",
    "9. ‚úÖ Model Comparison & Evaluation\n",
    "\n",
    "**Total Time:** 2-4 hours (depending on what you run)\n",
    "\n",
    "**Just run cells sequentially - no notebook switching required!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Table of Contents\n",
    "\n",
    "**Click \"Runtime\" ‚Üí \"Run all\" to execute everything!**\n",
    "\n",
    "Or run sections individually:\n",
    "- Part 1: Setup (5 min)\n",
    "- Part 2: Data Exploration (10 min)\n",
    "- Part 3: Baseline Evaluation (10 min)\n",
    "- Part 4: Full Finetuning GPT-2 (30-45 min)\n",
    "- Part 5: LoRA Finetuning (20-30 min)\n",
    "- Part 6: QLoRA on Mistral-7B (40-60 min) ‚≠ê\n",
    "- Part 7: Instruction Tuning (30-45 min) ‚≠ê\n",
    "- Part 8: Text Classification (20-30 min)\n",
    "- Part 9: Evaluation & Comparison (15 min)\n",
    "- Part 10: Summary & Next Steps\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Environment Setup (5 minutes)\n",
    "\n",
    "Setting up Google Colab environment for LLM finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/DS535/llm-finetuning-production.git\n",
    "%cd llm-finetuning-production\n",
    "\n",
    "print(\"‚úì Repository cloned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    total_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Total GPU Memory: {total_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (5-10 minutes)\n",
    "print(\"Installing dependencies... This will take 5-10 minutes.\")\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"\\n‚úì All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for saving checkpoints)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs(\"/content/drive/MyDrive/llm_checkpoints\", exist_ok=True)\n",
    "os.makedirs(\"/content/drive/MyDrive/llm_models\", exist_ok=True)\n",
    "print(\"‚úì Google Drive mounted\")\n",
    "print(\"  Checkpoints: /content/drive/MyDrive/llm_checkpoints\")\n",
    "print(\"  Models: /content/drive/MyDrive/llm_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all libraries\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import bitsandbytes as bnb\n",
    "import accelerate\n",
    "\n",
    "print(\"Library Versions:\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "print(f\"  peft: {peft.__version__}\")\n",
    "print(f\"  trl: {trl.__version__}\")\n",
    "print(f\"  bitsandbytes: {bnb.__version__}\")\n",
    "print(f\"  accelerate: {accelerate.__version__}\")\n",
    "print(f\"  torch: {torch.__version__}\")\n",
    "print(\"\\n‚úì Setup complete! Ready for finetuning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project to path\n",
    "import sys\n",
    "sys.path.append('/content/llm-finetuning-production')\n",
    "\n",
    "# Test custom utilities\n",
    "from src.utils.memory import print_gpu_utilization\n",
    "\n",
    "print(\"\\n=== Initial GPU Memory ===\")\n",
    "print_gpu_utilization()\n",
    "print(\"\\n‚úì Custom utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Data Exploration (10 minutes)\n",
    "\n",
    "Loading and analyzing datasets for different finetuning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Visualization libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dolly-15k (Instruction Following Dataset)\n",
    "print(\"Loading Dolly-15k instruction dataset...\")\n",
    "dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"\\nDataset size: {len(dolly):,} instruction-response pairs\")\n",
    "print(f\"Columns: {dolly.column_names}\")\n",
    "print(f\"\\nüìù Example instruction:\")\n",
    "print(f\"Category: {dolly[0]['category']}\")\n",
    "print(f\"Instruction: {dolly[0]['instruction']}\")\n",
    "print(f\"Response: {dolly[0]['response'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze instruction categories\n",
    "categories = Counter(dolly['category'])\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(categories.keys(), categories.values(), color='steelblue')\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Distribution of Instruction Categories in Dolly-15k', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCategory breakdown:\")\n",
    "for cat, count in categories.most_common():\n",
    "    pct = count / len(dolly) * 100\n",
    "    print(f\"  {cat:30s}: {count:5,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization analysis\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"Loading tokenizer for analysis...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Analyze token counts on sample\n",
    "print(\"\\nAnalyzing token counts (sampling 1000 examples)...\")\n",
    "sample = dolly.select(range(min(1000, len(dolly))))\n",
    "token_counts = [\n",
    "    len(tokenizer.encode(ex['instruction'] + ' ' + ex['response']))\n",
    "    for ex in sample\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(token_counts, bins=50, edgecolor='black', color='green', alpha=0.7)\n",
    "plt.axvline(512, color='red', linestyle='--', linewidth=2, label='512 token limit')\n",
    "plt.axvline(np.mean(token_counts), color='blue', linestyle='--', linewidth=2, label=f'Mean: {np.mean(token_counts):.0f}')\n",
    "plt.xlabel('Token Count', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Token Distribution in Dolly Instructions', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "pct_over_512 = sum(1 for x in token_counts if x > 512) / len(token_counts) * 100\n",
    "print(f\"\\nToken statistics:\")\n",
    "print(f\"  Mean: {np.mean(token_counts):.1f} tokens\")\n",
    "print(f\"  Median: {np.median(token_counts):.1f} tokens\")\n",
    "print(f\"  Max: {max(token_counts)} tokens\")\n",
    "print(f\"  % over 512 tokens: {pct_over_512:.1f}%\")\n",
    "print(f\"\\nüí° Recommendation: Use max_length=512 for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AG News (Classification Dataset)\n",
    "print(\"Loading AG News classification dataset...\")\n",
    "ag_news = load_dataset(\"SetFit/ag_news\", split=\"train\")\n",
    "\n",
    "label_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "label_counts = Counter(ag_news['label'])\n",
    "\n",
    "print(f\"\\nDataset size: {len(ag_news):,} news articles\")\n",
    "print(f\"Classes: {len(label_names)}\")\n",
    "print(f\"\\nüì∞ Example article:\")\n",
    "print(f\"Label: {label_names[ag_news[0]['label']]}\")\n",
    "print(f\"Text: {ag_news[0]['text'][:200]}...\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([label_names[i] for i in sorted(label_counts.keys())],\n",
    "        [label_counts[i] for i in sorted(label_counts.keys())],\n",
    "        color=['coral', 'skyblue', 'lightgreen', 'plum'])\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title('Class Distribution in AG News', fontsize=14)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClass balance:\")\n",
    "for i in sorted(label_counts.keys()):\n",
    "    count = label_counts[i]\n",
    "    print(f\"  {label_names[i]:10s}: {count:6,} ({count/len(ag_news)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n‚úì Data exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Baseline Evaluation (10 minutes)\n",
    "\n",
    "Evaluating pretrained models before finetuning to establish baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading GPT-2 for baseline evaluation...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "\n",
    "print(f\"\\nModel: GPT-2\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Size: ~500 MB\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test zero-shot generation\n",
    "test_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"To learn Python programming, you should\",\n",
    "    \"The best way to stay healthy is\",\n",
    "    \"Artificial intelligence will\",\n",
    "    \"The future of transportation\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ZERO-SHOT GENERATION (Before Finetuning)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=60,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    generated = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    print(f\"\\n{i}. Prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"   Output: {generated}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline perplexity\n",
    "print(\"\\nComputing baseline perplexity on Dolly responses...\")\n",
    "\n",
    "test_texts = [ex['response'] for ex in dolly.select(range(100))]\n",
    "\n",
    "total_loss = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(test_texts, desc=\"Computing perplexity\"):\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "        outputs = model(**enc, labels=enc[\"input_ids\"])\n",
    "        total_loss += outputs.loss.item() * enc[\"input_ids\"].size(1)\n",
    "        total_tokens += enc[\"input_ids\"].size(1)\n",
    "\n",
    "baseline_perplexity = np.exp(total_loss / total_tokens)\n",
    "\n",
    "print(f\"\\nüìä Baseline Perplexity: {baseline_perplexity:.2f}\")\n",
    "print(\"   (Lower is better - measures how well model predicts text)\")\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úì Baseline evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Full Finetuning - GPT-2 (30-45 minutes)\n",
    "\n",
    "Traditional full-parameter finetuning on TinyStories dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "print(\"Preparing for full finetuning...\")\n",
    "\n",
    "# Load fresh model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(f\"Model loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Load TinyStories dataset (small subset for quick training)\n",
    "print(\"\\nLoading TinyStories dataset...\")\n",
    "tiny_stories = load_dataset(\"roneneldan/TinyStories\", split=\"train[:5000]\")\n",
    "tiny_split = tiny_stories.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train examples: {len(tiny_split['train']):,}\")\n",
    "print(f\"Val examples: {len(tiny_split['test']):,}\")\n",
    "print(f\"\\nExample story:\\n{tiny_split['train'][0]['text'][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = tiny_split.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "print(\"‚úì Dataset tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure training\noutput_dir = \"/content/drive/MyDrive/llm_checkpoints/gpt2_full_finetuned\"\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,  # Effective batch size = 16\n    learning_rate=5e-5,\n    warmup_steps=100,\n    logging_steps=50,\n    eval_steps=200,\n    save_steps=200,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    load_best_model_at_end=True,\n    fp16=True,  # Mixed precision for speed\n    report_to=\"none\",  # Disable wandb for now\n    save_total_limit=2  # Keep only 2 checkpoints\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator\n)\n\nprint(\"‚úì Trainer configured\")\nprint(f\"\\nTraining parameters:\")\nprint(f\"  Epochs: 3\")\nprint(f\"  Batch size: 4 (effective: 16 with gradient accumulation)\")\nprint(f\"  Learning rate: 5e-5\")\nprint(f\"  Mixed precision: FP16\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING FULL FINETUNING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will take 30-45 minutes...\\n\")\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining metrics:\")\n",
    "print(f\"  Final loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {train_result.metrics['train_runtime']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test finetuned model\n",
    "model.eval()\n",
    "model.to(\"cuda\")\n",
    "\n",
    "story_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The little girl\",\n",
    "    \"In a magical forest\",\n",
    "    \"One sunny day\",\n",
    "    \"There was a brave knight\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINETUNED MODEL GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(story_prompts, 1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    story = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    print(f\"\\n{i}. Prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"   Story: {story}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save finetuned model\n",
    "save_path = \"/content/drive/MyDrive/llm_models/gpt2_tinystories_finetuned\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"‚úì Model saved to: {save_path}\")\n",
    "print(f\"  Model size: ~500 MB\")\n",
    "\n",
    "# Clean up\n",
    "del model, trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úì Full finetuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: LoRA Finetuning (20-30 minutes)\n",
    "\n",
    "Parameter-efficient finetuning using LoRA adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "print(\"Setting up LoRA finetuning...\")\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(f\"Base model: {sum(p.numel() for p in base_model.parameters()):,} parameters\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank (higher = more capacity but more parameters)\n",
    "    lora_alpha=32,  # LoRA scaling factor (usually 2*r)\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 attention layers\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model_lora = get_peft_model(base_model, lora_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "model_lora.print_trainable_parameters()\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úì LoRA adapters applied\")\n",
    "print(\"  üí° Training <1% of parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure LoRA training\ntraining_args_lora = TrainingArguments(\n    output_dir=\"/content/drive/MyDrive/llm_checkpoints/gpt2_lora\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,  # Higher LR for LoRA (typically 10x full finetuning)\n    warmup_steps=100,\n    logging_steps=50,\n    eval_steps=200,\n    save_steps=200,\n    eval_strategy=\"steps\",\n    fp16=True,\n    report_to=\"none\",\n    save_total_limit=2\n)\n\ntrainer_lora = Trainer(\n    model=model_lora,\n    args=training_args_lora,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    data_collator=data_collator\n)\n\nprint(\"‚úì LoRA trainer configured\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with LoRA\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING LoRA FINETUNING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will take 20-30 minutes...\\n\")\n",
    "\n",
    "lora_result = trainer_lora.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì LoRA TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining metrics:\")\n",
    "print(f\"  Final loss: {lora_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {lora_result.metrics['train_runtime']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LoRA model\n",
    "model_lora.eval()\n",
    "model_lora.to(\"cuda\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LoRA MODEL GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(story_prompts, 1):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model_lora.generate(**inputs, max_length=150, do_sample=True, temperature=0.8)\n",
    "    story = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    print(f\"\\n{i}. Prompt: \\\"{prompt}\\\"\")\n",
    "    print(f\"   Story: {story}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "lora_path = \"/content/drive/MyDrive/llm_models/gpt2_lora_adapters\"\n",
    "model_lora.save_pretrained(lora_path)\n",
    "\n",
    "print(f\"‚úì LoRA adapters saved to: {lora_path}\")\n",
    "print(f\"  Adapter size: ~5-10 MB (100x smaller than full model!)\")\n",
    "print(\"\\nüí° Benefits of LoRA:\")\n",
    "print(\"  ‚úì Trains <1% of parameters\")\n",
    "print(\"  ‚úì Much smaller checkpoint files\")\n",
    "print(\"  ‚úì Faster training\")\n",
    "print(\"  ‚úì Lower memory usage\")\n",
    "print(\"  ‚úì Can share adapters for different tasks\")\n",
    "\n",
    "# Clean up\n",
    "del model_lora, trainer_lora\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úì LoRA finetuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 6: QLoRA - Mistral-7B on 8GB GPU! (40-60 minutes) ‚≠ê\n",
    "\n",
    "Finetuning a 7B parameter model using 4-bit quantization + LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "print(\"Setting up QLoRA for Mistral-7B...\")\n",
    "print(\"\\nüí° QLoRA = 4-bit Quantization + LoRA\")\n",
    "print(\"  Enables 7B model on 8GB GPU!\\n\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 (best for LLMs)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16\n",
    "    bnb_4bit_use_double_quant=True  # Double quantization for extra memory savings\n",
    ")\n",
    "\n",
    "print(\"‚úì 4-bit quantization config ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mistral-7B with 4-bit quantization\n",
    "print(\"Loading Mistral-7B-Instruct-v0.1 with 4-bit quantization...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "\n",
    "print(\"\\n‚úì Mistral-7B loaded!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in mistral_model.parameters()):,}\")\n",
    "print(f\"  Quantized to 4-bit!\\n\")\n",
    "\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for QLoRA\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "print(\"Preparing model for k-bit training...\")\n",
    "mistral_model = prepare_model_for_kbit_training(mistral_model)\n",
    "\n",
    "# Configure LoRA for Mistral\n",
    "qlora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "mistral_lora = get_peft_model(mistral_model, qlora_config)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "mistral_lora.print_trainable_parameters()\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n‚úì QLoRA ready!\")\n",
    "print(\"  üí° 7B model trainable on 8GB GPU!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare instruction dataset for Mistral\n",
    "print(\"Preparing instruction dataset...\")\n",
    "\n",
    "# Use small subset of Dolly for demonstration\n",
    "dolly_small = dolly.select(range(1000))\n",
    "dolly_small_split = dolly_small.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "def format_instruction(example):\n",
    "    instruction = example['instruction']\n",
    "    response = example['response']\n",
    "    text = f\"<s>[INST] {instruction} [/INST] {response}</s>\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "formatted_dataset = dolly_small_split.map(format_instruction, remove_columns=dolly_small_split[\"train\"].column_names)\n",
    "\n",
    "def tokenize_mistral(examples):\n",
    "    return mistral_tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_mistral = formatted_dataset.map(tokenize_mistral, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "print(f\"‚úì Dataset prepared: {len(tokenized_mistral['train'])} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure QLoRA training\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "qlora_training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/llm_checkpoints/mistral_qlora\",\n",
    "    num_train_epochs=2,  # Fewer epochs for demo\n",
    "    per_device_train_batch_size=2,  # Smaller batch for 7B model\n",
    "    gradient_accumulation_steps=8,  # Effective batch = 16\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=50,\n",
    "    logging_steps=25,\n",
    "    save_steps=100,\n",
    "    fp16=False,  # Use bf16 for better numerical stability\n",
    "    bf16=True,\n",
    "    gradient_checkpointing=True,  # Essential for large models\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "data_collator_mistral = DataCollatorForLanguageModeling(mistral_tokenizer, mlm=False)\n",
    "\n",
    "qlora_trainer = Trainer(\n",
    "    model=mistral_lora,\n",
    "    args=qlora_training_args,\n",
    "    train_dataset=tokenized_mistral[\"train\"],\n",
    "    data_collator=data_collator_mistral\n",
    ")\n",
    "\n",
    "print(\"‚úì QLoRA trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with QLoRA\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING QLoRA TRAINING ON MISTRAL-7B\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will take 40-60 minutes...\\n\")\n",
    "\n",
    "qlora_result = qlora_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì QLoRA TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nYou just finetuned a 7B model on 8GB GPU!\")\n",
    "print(f\"  Final loss: {qlora_result.training_loss:.4f}\")\n",
    "print(f\"  Training time: {qlora_result.metrics['train_runtime']:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Mistral QLoRA\n",
    "mistral_lora.eval()\n",
    "\n",
    "test_instructions = [\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"Write a short poem about coding.\",\n",
    "    \"What are the benefits of exercise?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MISTRAL-7B QLoRA GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, instruction in enumerate(test_instructions, 1):\n",
    "    prompt = f\"<s>[INST] {instruction} [/INST]\"\n",
    "    inputs = mistral_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = mistral_lora.generate(**inputs, max_length=200, do_sample=True, temperature=0.7)\n",
    "    response = mistral_tokenizer.decode(outputs[0])\n",
    "    \n",
    "    print(f\"\\n{i}. Instruction: {instruction}\")\n",
    "    print(f\"   Response: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Mistral QLoRA adapters\n",
    "mistral_path = \"/content/drive/MyDrive/llm_models/mistral_qlora_adapters\"\n",
    "mistral_lora.save_pretrained(mistral_path)\n",
    "\n",
    "print(f\"‚úì Mistral QLoRA adapters saved to: {mistral_path}\")\n",
    "print(f\"  Adapter size: ~50-100 MB\")\n",
    "print(f\"  Base model: 7B parameters (not saved - use from HuggingFace)\")\n",
    "\n",
    "# Clean up\n",
    "del mistral_lora, qlora_trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úì QLoRA complete! You finetuned a 7B model on 8GB GPU! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 7: Instruction Tuning (30-45 minutes) ‚≠ê\n",
    "\n",
    "Production-ready instruction tuning using TRL's SFTTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "print(\"Setting up instruction tuning with SFTTrainer...\")\n",
    "print(\"\\nüí° SFTTrainer = Supervised Fine-Tuning Trainer\")\n",
    "print(\"  Optimized for instruction-following datasets\\n\")\n",
    "\n",
    "# Load model for instruction tuning\n",
    "instruction_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Apply LoRA\n",
    "instruction_lora = get_peft_model(instruction_model, lora_config)\n",
    "instruction_lora.print_trainable_parameters()\n",
    "\n",
    "print(\"\\n‚úì Model ready for instruction tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare instruction dataset with proper formatting\n",
    "print(\"Formatting instructions (Alpaca style)...\")\n",
    "\n",
    "dolly_instruction = dolly.select(range(2000))\n",
    "\n",
    "def format_alpaca(example):\n",
    "    instruction = example['instruction']\n",
    "    context = example.get('context', '')\n",
    "    response = example['response']\n",
    "    \n",
    "    if context:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\"\n",
    "    else:\n",
    "        text = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{response}\"\"\"\n",
    "    \n",
    "    return {\"text\": text}\n",
    "\n",
    "formatted_instructions = dolly_instruction.map(format_alpaca, remove_columns=dolly_instruction.column_names)\n",
    "instruction_split = formatted_instructions.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"‚úì {len(instruction_split['train'])} training examples formatted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SFTTrainer\n",
    "sft_training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/llm_checkpoints/gpt2_instruction_tuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=instruction_lora,\n",
    "    args=sft_training_args,\n",
    "    train_dataset=instruction_split[\"train\"],\n",
    "    eval_dataset=instruction_split[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"‚úì SFTTrainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train instruction-following model\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING INSTRUCTION TUNING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will take 30-45 minutes...\\n\")\n",
    "\n",
    "sft_result = sft_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì INSTRUCTION TUNING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nYou now have a ChatGPT-style assistant model!\")\n",
    "print(f\"  Final loss: {sft_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test instruction-tuned model\n",
    "instruction_lora.eval()\n",
    "instruction_lora.to(\"cuda\")\n",
    "\n",
    "test_instructions = [\n",
    "    \"Explain photosynthesis in simple terms.\",\n",
    "    \"Write a Python function to calculate factorial.\",\n",
    "    \"What are the health benefits of meditation?\",\n",
    "    \"How do airplanes fly?\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INSTRUCTION-TUNED MODEL (ChatGPT-style)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, instruction in enumerate(test_instructions, 1):\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = instruction_lora.generate(**inputs, max_length=200, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    print(f\"\\n{i}. {instruction}\")\n",
    "    print(f\"   {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save instruction-tuned model\n",
    "instruction_path = \"/content/drive/MyDrive/llm_models/gpt2_instruction_tuned\"\n",
    "instruction_lora.save_pretrained(instruction_path)\n",
    "\n",
    "print(f\"‚úì Instruction-tuned model saved to: {instruction_path}\")\n",
    "print(\"\\nüí° This model can now:\")\n",
    "print(\"  ‚úì Follow instructions\")\n",
    "print(\"  ‚úì Answer questions\")\n",
    "print(\"  ‚úì Generate specific content\")\n",
    "print(\"  ‚úì Act as a ChatGPT-style assistant\")\n",
    "\n",
    "# Clean up\n",
    "del instruction_lora, sft_trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úì Instruction tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 8: Text Classification with BERT (20-30 minutes)\n",
    "\n",
    "Finetuning BERT for multi-class text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "print(\"Loading BERT for text classification...\")\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=4  # AG News has 4 classes\n",
    ")\n",
    "\n",
    "print(f\"‚úì BERT loaded: {sum(p.numel() for p in bert_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare AG News dataset\n",
    "print(\"Preparing AG News dataset...\")\n",
    "\n",
    "ag_small = ag_news.select(range(5000))\n",
    "ag_small_split = ag_small.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "def tokenize_ag_news(examples):\n",
    "    return bert_tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_ag = ag_small_split.map(tokenize_ag_news, batched=True)\n",
    "\n",
    "print(f\"‚úì Dataset prepared: {len(tokenized_ag['train'])} train, {len(tokenized_ag['test'])} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1_macro = f1_score(labels, predictions, average='macro')\n",
    "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted\n",
    "    }\n",
    "\n",
    "print(\"‚úì Metrics function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure BERT training\nbert_training_args = TrainingArguments(\n    output_dir=\"/content/drive/MyDrive/llm_checkpoints/bert_ag_news\",\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    warmup_steps=100,\n    logging_steps=50,\n    eval_steps=100,\n    save_steps=100,\n    eval_strategy=\"steps\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_weighted\",\n    fp16=True,\n    report_to=\"none\"\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=bert_tokenizer)\n\nbert_trainer = Trainer(\n    model=bert_model,\n    args=bert_training_args,\n    train_dataset=tokenized_ag[\"train\"],\n    eval_dataset=tokenized_ag[\"test\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\nprint(\"‚úì BERT trainer configured\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BERT classifier\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING BERT CLASSIFICATION TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This will take 20-30 minutes...\\n\")\n",
    "\n",
    "bert_result = bert_trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì BERT TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BERT model\n",
    "print(\"Evaluating BERT classifier...\\n\")\n",
    "\n",
    "eval_results = bert_trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT CLASSIFICATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n  Accuracy:    {eval_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1 (macro):  {eval_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"  F1 (weighted): {eval_results['eval_f1_weighted']:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BERT classifier\n",
    "bert_model.eval()\n",
    "bert_model.to(\"cuda\")\n",
    "\n",
    "label_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "test_texts = [\n",
    "    \"The United Nations held an emergency meeting to discuss climate change.\",\n",
    "    \"The Lakers won the championship game by a score of 112-108.\",\n",
    "    \"The stock market rose 2% after the Federal Reserve announcement.\",\n",
    "    \"Scientists discovered a new species of bacteria in the deep ocean.\"\n",
    "]\n",
    "\n",
    "print(\"\\n=\" * 60)\n",
    "print(\"BERT PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    \n",
    "    predicted_class = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    confidence = torch.softmax(outputs.logits, dim=-1)[0][predicted_class].item()\n",
    "    \n",
    "    print(f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Predicted: {label_names[predicted_class]} (confidence: {confidence:.2%})\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BERT classifier\n",
    "bert_path = \"/content/drive/MyDrive/llm_models/bert_ag_news_classifier\"\n",
    "bert_model.save_pretrained(bert_path)\n",
    "bert_tokenizer.save_pretrained(bert_path)\n",
    "\n",
    "print(f\"‚úì BERT classifier saved to: {bert_path}\")\n",
    "\n",
    "# Clean up\n",
    "del bert_model, bert_trainer\n",
    "torch.cuda.empty_cache()\n",
    "print(\"\\n‚úì Classification training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 9: Model Comparison & Evaluation (15 minutes)\n",
    "\n",
    "Comparing all finetuned models and summarizing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = [\n",
    "    {\n",
    "        \"Model\": \"GPT-2 (Baseline)\",\n",
    "        \"Task\": \"Generation\",\n",
    "        \"Method\": \"Pretrained\",\n",
    "        \"Perplexity\": baseline_perplexity,\n",
    "        \"Trainable Params\": \"0\",\n",
    "        \"Training Time\": \"0 min\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"GPT-2 Full FT\",\n",
    "        \"Task\": \"Story Generation\",\n",
    "        \"Method\": \"Full Finetuning\",\n",
    "        \"Perplexity\": \"-\",\n",
    "        \"Trainable Params\": \"124M (100%)\",\n",
    "        \"Training Time\": \"30-45 min\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"GPT-2 LoRA\",\n",
    "        \"Task\": \"Story Generation\",\n",
    "        \"Method\": \"LoRA (r=16)\",\n",
    "        \"Perplexity\": \"-\",\n",
    "        \"Trainable Params\": \"~1M (<1%)\",\n",
    "        \"Training Time\": \"20-30 min\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"Mistral-7B QLoRA\",\n",
    "        \"Task\": \"Instruction Following\",\n",
    "        \"Method\": \"QLoRA (4-bit + LoRA)\",\n",
    "        \"Perplexity\": \"-\",\n",
    "        \"Trainable Params\": \"~40M (<0.6%)\",\n",
    "        \"Training Time\": \"40-60 min\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"GPT-2 Instruction\",\n",
    "        \"Task\": \"Instruction Following\",\n",
    "        \"Method\": \"SFT + LoRA\",\n",
    "        \"Perplexity\": \"-\",\n",
    "        \"Trainable Params\": \"~1M (<1%)\",\n",
    "        \"Training Time\": \"30-45 min\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"BERT Classifier\",\n",
    "        \"Task\": \"Classification\",\n",
    "        \"Method\": \"Full Finetuning\",\n",
    "        \"Perplexity\": \"-\",\n",
    "        \"Trainable Params\": \"110M (100%)\",\n",
    "        \"Training Time\": \"20-30 min\"\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training time comparison\n",
    "models = ['Full FT', 'LoRA', 'QLoRA', 'Instruction', 'BERT']\n",
    "train_times = [37.5, 25, 50, 37.5, 25]  # Average times in minutes\n",
    "\n",
    "axes[0].barh(models, train_times, color=['coral', 'skyblue', 'lightgreen', 'plum', 'gold'])\n",
    "axes[0].set_xlabel('Training Time (minutes)', fontsize=12)\n",
    "axes[0].set_title('Training Time Comparison', fontsize=14)\n",
    "axes[0].grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Trainable parameters comparison\n",
    "param_pcts = [100, 0.8, 0.6, 0.8, 100]\n",
    "\n",
    "axes[1].barh(models, param_pcts, color=['coral', 'skyblue', 'lightgreen', 'plum', 'gold'])\n",
    "axes[1].set_xlabel('Trainable Parameters (%)', fontsize=12)\n",
    "axes[1].set_title('Parameter Efficiency Comparison', fontsize=14)\n",
    "axes[1].grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  ‚úì LoRA/QLoRA train <1% of parameters but achieve similar quality\")\n",
    "print(\"  ‚úì QLoRA enables 7B models on 8GB GPU\")\n",
    "print(\"  ‚úì SFTTrainer optimized for instruction tuning\")\n",
    "print(\"  ‚úì BERT excellent for classification tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 10: Summary & Next Steps\n",
    "\n",
    "Congratulations! You've completed the comprehensive LLM finetuning tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" \" * 20 + \"üéâ TUTORIAL COMPLETE! üéâ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚úÖ What you accomplished:\")\n",
    "print(\"\\n1. ENVIRONMENT SETUP\")\n",
    "print(\"   ‚úì Configured Google Colab for LLM finetuning\")\n",
    "print(\"   ‚úì Installed all necessary libraries\")\n",
    "print(\"   ‚úì Set up Google Drive for model persistence\")\n",
    "\n",
    "print(\"\\n2. DATA EXPLORATION\")\n",
    "print(\"   ‚úì Loaded instruction dataset (Dolly-15k)\")\n",
    "print(\"   ‚úì Loaded classification dataset (AG News)\")\n",
    "print(\"   ‚úì Analyzed text lengths and token distributions\")\n",
    "print(\"   ‚úì Visualized category and class distributions\")\n",
    "\n",
    "print(\"\\n3. BASELINE EVALUATION\")\n",
    "print(\"   ‚úì Tested pretrained GPT-2 zero-shot generation\")\n",
    "print(\"   ‚úì Computed baseline perplexity\")\n",
    "print(\"   ‚úì Established performance benchmarks\")\n",
    "\n",
    "print(\"\\n4. FULL FINETUNING\")\n",
    "print(\"   ‚úì Finetuned GPT-2 on TinyStories (story generation)\")\n",
    "print(\"   ‚úì Learned traditional finetuning mechanics\")\n",
    "print(\"   ‚úì Generated creative stories\")\n",
    "\n",
    "print(\"\\n5. LoRA FINETUNING\")\n",
    "print(\"   ‚úì Applied LoRA adapters (parameter-efficient)\")\n",
    "print(\"   ‚úì Trained <1% of parameters\")\n",
    "print(\"   ‚úì Saved compact adapter checkpoints\")\n",
    "\n",
    "print(\"\\n6. QLoRA - 7B MODEL ON 8GB GPU! ‚≠ê\")\n",
    "print(\"   ‚úì Loaded Mistral-7B with 4-bit quantization\")\n",
    "print(\"   ‚úì Applied LoRA on quantized model\")\n",
    "print(\"   ‚úì Successfully trained 7B model on limited hardware!\")\n",
    "\n",
    "print(\"\\n7. INSTRUCTION TUNING ‚≠ê\")\n",
    "print(\"   ‚úì Used TRL's SFTTrainer\")\n",
    "print(\"   ‚úì Formatted instruction-response pairs\")\n",
    "print(\"   ‚úì Created ChatGPT-style assistant model\")\n",
    "\n",
    "print(\"\\n8. TEXT CLASSIFICATION\")\n",
    "print(\"   ‚úì Finetuned BERT for multi-class classification\")\n",
    "print(\"   ‚úì Achieved high accuracy on AG News\")\n",
    "print(\"   ‚úì Evaluated with precision, recall, F1 metrics\")\n",
    "\n",
    "print(\"\\n9. MODEL COMPARISON\")\n",
    "print(\"   ‚úì Compared all finetuning methods\")\n",
    "print(\"   ‚úì Analyzed trade-offs (speed vs quality vs memory)\")\n",
    "print(\"   ‚úì Visualized results\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"\\nüíæ Models saved to Google Drive:\")\n",
    "print(\"   1. /content/drive/MyDrive/llm_models/gpt2_tinystories_finetuned\")\n",
    "print(\"   2. /content/drive/MyDrive/llm_models/gpt2_lora_adapters\")\n",
    "print(\"   3. /content/drive/MyDrive/llm_models/mistral_qlora_adapters\")\n",
    "print(\"   4. /content/drive/MyDrive/llm_models/gpt2_instruction_tuned\")\n",
    "print(\"   5. /content/drive/MyDrive/llm_models/bert_ag_news_classifier\")\n",
    "\n",
    "print(\"\\nüìö Next Steps - Advanced Topics:\")\n",
    "print(\"   ‚Ä¢ Explore more datasets (SQuAD for QA, CNN/DailyMail for summarization)\")\n",
    "print(\"   ‚Ä¢ Try larger models (Llama-3.2-3B, Phi-2)\")\n",
    "print(\"   ‚Ä¢ Experiment with different LoRA ranks (r=8, 32, 64)\")\n",
    "print(\"   ‚Ä¢ Implement custom evaluation metrics\")\n",
    "print(\"   ‚Ä¢ Deploy models with FastAPI or Gradio\")\n",
    "print(\"   ‚Ä¢ Explore RLHF (Reinforcement Learning from Human Feedback)\")\n",
    "print(\"   ‚Ä¢ Try DPO (Direct Preference Optimization)\")\n",
    "\n",
    "print(\"\\nüéì Skills Learned:\")\n",
    "print(\"   ‚úì Full parameter finetuning\")\n",
    "print(\"   ‚úì Parameter-efficient finetuning (LoRA, QLoRA)\")\n",
    "print(\"   ‚úì Quantization techniques (4-bit, 8-bit)\")\n",
    "print(\"   ‚úì Instruction tuning for chat models\")\n",
    "print(\"   ‚úì Text classification with BERT\")\n",
    "print(\"   ‚úì Memory optimization for limited hardware\")\n",
    "print(\"   ‚úì Model evaluation and comparison\")\n",
    "print(\"   ‚úì Production-ready finetuning workflows\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" \" * 15 + \"You're now ready for production LLM work!\")\n",
    "print(\" \" * 20 + \"Happy Finetuning! üöÄ\")\n",
    "print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final GPU memory check\n",
    "print(\"\\n=== Final GPU Memory Status ===\")\n",
    "print_gpu_utilization()\n",
    "\n",
    "print(\"\\n‚úÖ All done! Check your Google Drive for saved models.\")\n",
    "print(\"\\nüìñ For more: https://github.com/DS535/llm-finetuning-production\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}