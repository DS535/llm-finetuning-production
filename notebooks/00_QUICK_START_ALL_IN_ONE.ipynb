{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Finetuning Quick Start - All-in-One Notebook\n",
    "\n",
    "This notebook contains complete, executable code to get you started with LLM finetuning immediately.\n",
    "\n",
    "**What's included:**\n",
    "1. Environment setup\n",
    "2. Data exploration\n",
    "3. Baseline evaluation\n",
    "4. Full finetuning (GPT-2)\n",
    "5. LoRA finetuning\n",
    "\n",
    "**Time:** 1-2 hours total\n",
    "\n",
    "**Just run all cells sequentially!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup (5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/DS535/llm-finetuning-production.git\n",
    "%cd llm-finetuning-production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (5-10 minutes)\n",
    "!pip install -q -r requirements.txt\n",
    "print(\"âœ“ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "os.makedirs(\"/content/drive/MyDrive/llm_checkpoints\", exist_ok=True)\n",
    "print(\"âœ“ Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import torch\n",
    "\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"datasets: {datasets.__version__}\")\n",
    "print(f\"peft: {peft.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"\\nâœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data Exploration (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load instruction dataset\n",
    "print(\"Loading Dolly-15k...\")\n",
    "dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "print(f\"Loaded {len(dolly):,} examples\")\n",
    "print(f\"\\nFirst example:\\n{dolly[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "from collections import Counter\n",
    "\n",
    "categories = Counter(dolly['category'])\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.bar(categories.keys(), categories.values())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Instruction Categories')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop categories:\")\n",
    "for cat, count in categories.most_common(5):\n",
    "    print(f\"  {cat}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization analysis\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Sample 1000 examples\n",
    "sample = dolly.select(range(1000))\n",
    "token_counts = [\n",
    "    len(tokenizer.encode(ex['instruction'] + ' ' + ex['response']))\n",
    "    for ex in sample\n",
    "]\n",
    "\n",
    "plt.hist(token_counts, bins=50)\n",
    "plt.axvline(512, color='red', linestyle='--', label='512 limit')\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Token Distribution (Mean: {np.mean(token_counts):.0f})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "pct_over_512 = sum(1 for x in token_counts if x > 512) / len(token_counts) * 100\n",
    "print(f\"\\nExamples > 512 tokens: {pct_over_512:.1f}%\")\n",
    "print(\"âœ“ Data exploration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Baseline Evaluation (10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load GPT-2\n",
    "print(\"Loading GPT-2...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "test_prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"To learn Python, you should\",\n",
    "    \"The best way to stay healthy is\"\n",
    "]\n",
    "\n",
    "print(\"Zero-shot generation:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=50, do_sample=True, temperature=0.7)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Output: {tokenizer.decode(outputs[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute baseline perplexity\n",
    "model.eval()\n",
    "test_texts = [ex['response'] for ex in dolly.select(range(100))]\n",
    "\n",
    "total_loss = 0\n",
    "total_tokens = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for text in tqdm(test_texts, desc=\"Computing perplexity\"):\n",
    "        enc = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512).to(\"cuda\")\n",
    "        outputs = model(**enc, labels=enc[\"input_ids\"])\n",
    "        total_loss += outputs.loss.item() * enc[\"input_ids\"].size(1)\n",
    "        total_tokens += enc[\"input_ids\"].size(1)\n",
    "\n",
    "baseline_perplexity = np.exp(total_loss / total_tokens)\n",
    "print(f\"\\nBaseline Perplexity: {baseline_perplexity:.2f}\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"âœ“ Baseline established\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Full Finetuning GPT-2 (30-45 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Load fresh model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prepare small dataset for quick training\n",
    "tiny_stories = load_dataset(\"roneneldan/TinyStories\", split=\"train[:5000]\")\n",
    "tiny_split = tiny_stories.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "print(f\"Train: {len(tiny_split['train'])}, Val: {len(tiny_split['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized = tiny_split.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(\"âœ“ Data tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/llm_checkpoints/gpt2_finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"âœ“ Trainer configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test finetuned model\n",
    "model.eval()\n",
    "\n",
    "story_prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"The little girl\",\n",
    "    \"In a magical forest\"\n",
    "]\n",
    "\n",
    "print(\"\\nFinetuned generation:\\n\")\n",
    "for prompt in story_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_length=100, do_sample=True, temperature=0.7)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Story: {tokenizer.decode(outputs[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_path = \"/content/drive/MyDrive/llm_models/gpt2_tinystories\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"âœ“ Model saved to {save_path}\")\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: LoRA Finetuning (20-30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load model for LoRA\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2 attention modules\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model_lora = get_peft_model(base_model, lora_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ“ LoRA applied\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with LoRA\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/llm_checkpoints/gpt2_lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,  # Higher LR for LoRA\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA training...\")\n",
    "trainer_lora.train()\n",
    "print(\"\\nâœ“ LoRA training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LoRA model\n",
    "model_lora.eval()\n",
    "\n",
    "print(\"\\nLoRA generation:\\n\")\n",
    "for prompt in story_prompts:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model_lora.generate(**inputs, max_length=100)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Story: {tokenizer.decode(outputs[0])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "lora_path = \"/content/drive/MyDrive/llm_models/gpt2_lora_adapters\"\n",
    "model_lora.save_pretrained(lora_path)\n",
    "print(f\"âœ“ LoRA adapters saved to {lora_path}\")\n",
    "print(\"  (Only adapters saved - much smaller than full model!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"QUICK START COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nâœ“ What you learned:\")\n",
    "print(\"  1. Environment setup for LLM finetuning\")\n",
    "print(\"  2. Dataset loading and exploration\")\n",
    "print(\"  3. Baseline evaluation\")\n",
    "print(\"  4. Full parameter finetuning\")\n",
    "print(\"  5. LoRA (parameter-efficient finetuning)\")\n",
    "\n",
    "print(\"\\nâœ“ Models saved:\")\n",
    "print(\"  - /content/drive/MyDrive/llm_models/gpt2_tinystories\")\n",
    "print(\"  - /content/drive/MyDrive/llm_models/gpt2_lora_adapters\")\n",
    "\n",
    "print(\"\\nðŸ“š Next steps:\")\n",
    "print(\"  - Notebook 06: LoRA on larger models (Llama, Phi-2)\")\n",
    "print(\"  - Notebook 07: QLoRA on Mistral-7B (4-bit quantization)\")\n",
    "print(\"  - Notebook 09: Production instruction tuning\")\n",
    "print(\"  - Notebook 14: Model comparison and evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Great job! You're now ready for advanced finetuning!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
