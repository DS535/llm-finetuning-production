{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Environment Setup\n",
    "\n",
    "This notebook sets up the complete environment for LLM finetuning in Google Colab.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. Check GPU availability and specs\n",
    "2. Mount Google Drive for checkpoint persistence\n",
    "3. Clone GitHub repository\n",
    "4. Install all required dependencies\n",
    "5. Configure experiment tracking (W&B, TensorBoard)\n",
    "6. Verify installation\n",
    "7. Test basic functionality\n",
    "\n",
    "**Estimated time:** 5-10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úì Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚úó Not running in Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} GB\")\n",
    "    \n",
    "    # Determine GPU type\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'T4' in gpu_name:\n",
    "        print(\"\\nüìå GPU Type: Tesla T4 (Good for most models with quantization)\")\n",
    "    elif 'V100' in gpu_name:\n",
    "        print(\"\\nüìå GPU Type: Tesla V100 (Excellent for all models)\")\n",
    "    elif 'A100' in gpu_name:\n",
    "        print(\"\\nüìå GPU Type: A100 (Best performance, can use bf16)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU detected!\")\n",
    "    print(\"Go to: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úì Google Drive mounted\")\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    import os\n",
    "    checkpoint_dir = \"/content/drive/MyDrive/llm_finetuning_checkpoints\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(f\"‚úì Checkpoint directory: {checkpoint_dir}\")\n",
    "else:\n",
    "    print(\"Skipping Drive mount (not in Colab)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clone GitHub Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your GitHub repository URL\n",
    "REPO_URL = \"https://github.com/YOUR_USERNAME/llm-finetuning-production.git\"\n",
    "\n",
    "# Clone repo if not already present\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"llm-finetuning-production\"):\n",
    "    !git clone {REPO_URL}\n",
    "    print(\"‚úì Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úì Repository already exists\")\n",
    "\n",
    "# Change to project directory\n",
    "%cd llm-finetuning-production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all requirements\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "print(\"‚úì Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify key libraries\n",
    "import transformers\n",
    "import datasets\n",
    "import peft\n",
    "import trl\n",
    "import accelerate\n",
    "import evaluate\n",
    "\n",
    "print(\"Library Versions:\")\n",
    "print(f\"  transformers: {transformers.__version__}\")\n",
    "print(f\"  datasets: {datasets.__version__}\")\n",
    "print(f\"  peft: {peft.__version__}\")\n",
    "print(f\"  trl: {trl.__version__}\")\n",
    "print(f\"  accelerate: {accelerate.__version__}\")\n",
    "\n",
    "# Check bitsandbytes (for quantization)\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    print(f\"  bitsandbytes: {bnb.__version__}\")\n",
    "except:\n",
    "    print(\"  bitsandbytes: ‚úó Not available (quantization will fail)\")\n",
    "\n",
    "print(\"\\n‚úì All core libraries installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Weights & Biases\n",
    "import wandb\n",
    "\n",
    "# Option 1: Login with API key\n",
    "# wandb.login(key=\"YOUR_WANDB_API_KEY\")\n",
    "\n",
    "# Option 2: Login interactively\n",
    "# wandb.login()\n",
    "\n",
    "print(\"üìä W&B setup: Run wandb.login() when ready to track experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure HuggingFace (for downloading gated models)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Option 1: Login with token\n",
    "# login(token=\"YOUR_HF_TOKEN\")\n",
    "\n",
    "# Option 2: Login interactively\n",
    "# login()\n",
    "\n",
    "print(\"ü§ó HuggingFace setup: Run login() for gated models (Llama, Mistral, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Basic Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our custom modules\n",
    "import sys\n",
    "sys.path.append('/content/llm-finetuning-production')\n",
    "\n",
    "from src.utils.memory import print_gpu_utilization, get_gpu_memory_info\n",
    "\n",
    "print(\"\\n=== GPU Memory Status ===\\n\")\n",
    "print_gpu_utilization()\n",
    "\n",
    "mem_info = get_gpu_memory_info()\n",
    "print(f\"\\nAvailable Memory: {mem_info['available']:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading a small model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Testing model loading with GPT-2 (small)...\\n\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(\"cuda\")\n",
    "\n",
    "print(f\"‚úì Model loaded: {model_name}\")\n",
    "print(f\"‚úì Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test generation\n",
    "text = \"Hello, I am\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_length=20)\n",
    "generated = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"\\nTest generation:\")\n",
    "print(f\"  Input: {text}\")\n",
    "print(f\"  Output: {generated}\")\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "del tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n‚úì Model loading and generation test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Environment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n‚úì GPU: Available and tested\")\n",
    "print(\"‚úì Dependencies: Installed\")\n",
    "print(\"‚úì Custom modules: Working\")\n",
    "print(\"‚úì Model loading: Tested\")\n",
    "\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"  1. (Optional) Configure W&B: wandb.login()\")\n",
    "print(\"  2. (Optional) Configure HF: huggingface_hub.login()\")\n",
    "print(\"  3. Move to Notebook 02: Data Exploration\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
